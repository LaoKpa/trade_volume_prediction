{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to use Deep learning to predict future trade Volume.\n",
    "\n",
    "In this notebook, we will usually use normalized features and volume. The volume will be denormalized only for the purpose of model evaluation. We will use StandardScaler for normalization.\n",
    "\n",
    "We can draw conclusion from the Exploratory Data Analysis that not all features from the original dataset are usefull for prediction and few more additional features can be added. Models in this notebook will work with usually with following columns:\n",
    "\n",
    "| Feature  | Type | Description |\n",
    "| ------------- | ------------- ||\n",
    "| Volume X  | float64  |Historial trading volume shifted by X|\n",
    "| AdjCloseDiff X  | float64  |Historical difference between AdjClose price of two consecutive days shifted by X|\n",
    "| HighLowDiff X  | float64  ||Historical difference between High and Low price shifted by X|\n",
    "| DayOfWeek X  | one-hot |One-hot encoding value for each day|\n",
    "| Month X  | one-hot  |One-hot encoding value for each month|\n",
    "\n",
    "Le'ts now read the normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "sys.path.append(\"../\") # go to parent dir\n",
    "from util.read_data import DataReader\n",
    "from util.evaluator import ModelEvaluator\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test and train data\n",
    "reader = DataReader()\n",
    "# \n",
    "df = reader.read_normalized_data_for_rnn('../data/S&P500.csv')\n",
    "train_features, train_volume = reader.get_train_data(df)\n",
    "test_features, test_volume = reader.get_test_data(df)\n",
    "evaluator = ModelEvaluator(reader.label_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, features, volume):\n",
    "        self.features = features\n",
    "        self.volume = volume\n",
    "\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_volume = []\n",
    "        for i in range(batch_size):\n",
    "            # the training example in batch has to be random \n",
    "            index = np.random.randint(0, len(self.volume) - seq_length)\n",
    "            seq.append(self.features[index:index+seq_length].values)\n",
    "            next_volume.append(self.volume[index+seq_length])\n",
    "        return np.array(seq), np.array(next_volume).reshape(-1, 1)\n",
    "\n",
    "# TODO should be refactored to predict in batches, not one by one\n",
    "def predict_volume(rnn_model, features):\n",
    "    volume_pred = []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        x = features[i:i + seq_length]\n",
    "        x = x.values.reshape(1, seq_length, x.shape[1])\n",
    "        volume_pred.append(rnn_model(x).numpy()[0, 0])\n",
    "    return pd.Series(volume_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # neither deep nor shallow\n",
    "        self.cell1 = tf.nn.rnn_cell.BasicLSTMCell(num_units=512)\n",
    "        self.cell2 = tf.nn.rnn_cell.BasicLSTMCell(num_units=512)\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size, seq_length, _ = tf.shape(inputs)\n",
    "        state1 = self.cell1.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        state2 = self.cell2.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "        for t in range(seq_length.numpy()):\n",
    "            output, state1 = self.cell1(inputs[:, t, :], state1)\n",
    "            output, state2 = self.cell2(output, state2)\n",
    "            output = self.dense1(output)\n",
    "            output = self.dense2(output)\n",
    "        return output\n",
    "\n",
    "    def predict(self, inputs, temperature=1.):\n",
    "        batch_size, _ = tf.shape(inputs)\n",
    "        output = self(inputs)\n",
    "        return output.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use following hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "seq_length = 50\n",
    "num_batches = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 1.220601\n",
      "batch 100: loss 0.599872\n",
      "batch 200: loss 0.372579\n",
      "batch 300: loss 0.274882\n",
      "batch 400: loss 0.178969\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(train_features,train_volume)\n",
    "model = RNN()\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.losses.mean_squared_error(labels=y, predictions=y_pred)\n",
    "        if (batch_index % 100 == 0):\n",
    "            print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm 50 on train: MSE = 9.158586e+17, R2 = 0.624, conf. int. 95% of error = (208,790,216 - 266,491,325)\n",
      "lstm 50 on test: MSE = 9.636852e+17, R2 = -1.336, conf. int. 95% of error = (677,053,005 - 858,256,010)\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(\"lstm {} on train\".format(seq_length), train_volume[seq_length:], \n",
    "                         predict_volume(model, train_features)))\n",
    "\n",
    "print(evaluator.evaluate(\"lstm {} on test\".format(seq_length), test_volume[seq_length:], \n",
    "                         predict_volume(model, test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model is not gread, probably for similar reasons as mentioned in the machine_learning notebook - features used are not the best to predict future Volume.\n",
    "Still, some hyper-parameter tuning would help a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
